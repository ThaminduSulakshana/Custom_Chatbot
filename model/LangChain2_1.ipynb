{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cMgHPi0h6vag"
   },
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Obtaining dependency information for langchain from https://files.pythonhosted.org/packages/c0/f8/7801cd1bb059d8fc685bf2edd10360613a28d249b029f39d2c89a58eff23/langchain-0.0.349-py3-none-any.whl.metadata\n",
      "  Using cached langchain-0.0.349-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\sulayako\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Obtaining dependency information for SQLAlchemy<3,>=1.4 from https://files.pythonhosted.org/packages/d3/8a/321205f6ab88307618650f916f7c04f51864cd716c9583a25230ace70dc3/SQLAlchemy-2.0.23-cp312-cp312-win_amd64.whl.metadata\n",
      "  Using cached SQLAlchemy-2.0.23-cp312-cp312-win_amd64.whl.metadata (9.8 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
      "  Obtaining dependency information for aiohttp<4.0.0,>=3.8.3 from https://files.pythonhosted.org/packages/4e/13/e929a6a50288e60ade3961b294d2f5aeb251b6579e4290a5397e484d0df9/aiohttp-3.9.1-cp312-cp312-win_amd64.whl.metadata\n",
      "  Using cached aiohttp-3.9.1-cp312-cp312-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
      "  Obtaining dependency information for dataclasses-json<0.7,>=0.5.7 from https://files.pythonhosted.org/packages/ae/53/8c006de775834cd4ea64a445402dc195caeebb77dc76b7defb9b3887cb0d/dataclasses_json-0.6.3-py3-none-any.whl.metadata\n",
      "  Using cached dataclasses_json-0.6.3-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
      "  Obtaining dependency information for jsonpatch<2.0,>=1.33 from https://files.pythonhosted.org/packages/73/07/02e16ed01e04a374e644b575638ec7987ae846d25ad97bcc9945a3ee4b0e/jsonpatch-1.33-py2.py3-none-any.whl.metadata\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langchain-community<0.1,>=0.0.1 (from langchain)\n",
      "  Obtaining dependency information for langchain-community<0.1,>=0.0.1 from https://files.pythonhosted.org/packages/3e/73/2c1d5859d918b7a47c72f35dc407930f1f9754f9063cdfa29061b118ccbb/langchain_community-0.0.1-py3-none-any.whl.metadata\n",
      "  Using cached langchain_community-0.0.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain-core<0.1,>=0.0.13 (from langchain)\n",
      "  Obtaining dependency information for langchain-core<0.1,>=0.0.13 from https://files.pythonhosted.org/packages/96/30/1d6b35a757d698da75b24f8db74132b28de9fd964308e0bc8a2fe8d7be49/langchain_core-0.0.13-py3-none-any.whl.metadata\n",
      "  Using cached langchain_core-0.0.13-py3-none-any.whl.metadata (978 bytes)\n",
      "Collecting langsmith<0.1.0,>=0.0.63 (from langchain)\n",
      "  Obtaining dependency information for langsmith<0.1.0,>=0.0.63 from https://files.pythonhosted.org/packages/ea/09/e1458ea0a26037740aac27319479aaa79053a06413b5d715d56d37371b55/langsmith-0.0.69-py3-none-any.whl.metadata\n",
      "  Using cached langsmith-0.0.69-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting numpy<2,>=1 (from langchain)\n",
      "  Obtaining dependency information for numpy<2,>=1 from https://files.pythonhosted.org/packages/28/75/3b679b41713bb60e2e8f6e2f87be72c971c9e718b1c17b8f8749240ddca8/numpy-1.26.2-cp312-cp312-win_amd64.whl.metadata\n",
      "  Using cached numpy-1.26.2-cp312-cp312-win_amd64.whl.metadata (61 kB)\n",
      "Collecting pydantic<3,>=1 (from langchain)\n",
      "  Obtaining dependency information for pydantic<3,>=1 from https://files.pythonhosted.org/packages/0a/2b/64066de1c4cf3d4ed623beeb3bbf3f8d0cc26661f1e7d180ec5eb66b75a5/pydantic-2.5.2-py3-none-any.whl.metadata\n",
      "  Using cached pydantic-2.5.2-py3-none-any.whl.metadata (65 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\sulayako\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Collecting tenacity<9.0.0,>=8.1.0 (from langchain)\n",
      "  Obtaining dependency information for tenacity<9.0.0,>=8.1.0 from https://files.pythonhosted.org/packages/f4/f1/990741d5bb2487d529d20a433210ffa136a367751e454214013b441c4575/tenacity-8.2.3-py3-none-any.whl.metadata\n",
      "  Using cached tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\sulayako\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached multidict-6.0.4.tar.gz (51 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Obtaining dependency information for yarl<2.0,>=1.0 from https://files.pythonhosted.org/packages/c5/f4/2fdc5a11503bc61818243653d836061c9ce0370e2dd9ac5917258a007675/yarl-1.9.4-cp312-cp312-win_amd64.whl.metadata\n",
      "  Using cached yarl-1.9.4-cp312-cp312-win_amd64.whl.metadata (32 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached frozenlist-1.4.0.tar.gz (90 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Obtaining dependency information for marshmallow<4.0.0,>=3.18.0 from https://files.pythonhosted.org/packages/ed/3c/cebfdcad015240014ff08b883d1c0c427f2ba45ae8c6572851b6ef136cad/marshmallow-3.20.1-py3-none-any.whl.metadata\n",
      "  Using cached marshmallow-3.20.1-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Obtaining dependency information for typing-inspect<1,>=0.4.0 from https://files.pythonhosted.org/packages/65/f3/107a22063bf27bdccf2024833d3445f4eea42b2e598abfbd46f6a63b6cb0/typing_inspect-0.9.0-py3-none-any.whl.metadata\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\sulayako\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: anyio<5,>=3 in c:\\users\\sulayako\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.1,>=0.0.13->langchain) (4.1.0)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\sulayako\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.1,>=0.0.13->langchain) (23.2)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1->langchain)\n",
      "  Obtaining dependency information for annotated-types>=0.4.0 from https://files.pythonhosted.org/packages/28/78/d31230046e58c207284c6b2c4e8d96e6d3cb4e52354721b944d3e1ee4aa5/annotated_types-0.6.0-py3-none-any.whl.metadata\n",
      "  Using cached annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.14.5 (from pydantic<3,>=1->langchain)\n",
      "  Obtaining dependency information for pydantic-core==2.14.5 from https://files.pythonhosted.org/packages/60/5a/3161e1a1c138407cd2037b12ecdbe29f4890ccda1c0a0be69438c7d0065d/pydantic_core-2.14.5-cp312-none-win_amd64.whl.metadata\n",
      "  Using cached pydantic_core-2.14.5-cp312-none-win_amd64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\sulayako\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=1->langchain) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sulayako\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sulayako\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sulayako\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sulayako\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain) (2023.11.17)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Obtaining dependency information for greenlet!=0.4.17 from https://files.pythonhosted.org/packages/8f/72/5903f9bc1eb562daa836a48ae075e37850e6416856665f740a7e9301d22c/greenlet-3.0.2-cp312-cp312-win_amd64.whl.metadata\n",
      "  Using cached greenlet-3.0.2-cp312-cp312-win_amd64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\sulayako\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from anyio<5,>=3->langchain-core<0.1,>=0.0.13->langchain) (1.3.0)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Using cached langchain-0.0.349-py3-none-any.whl (808 kB)\n",
      "Using cached aiohttp-3.9.1-cp312-cp312-win_amd64.whl (362 kB)\n",
      "Using cached dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached langchain_community-0.0.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached langchain_core-0.0.13-py3-none-any.whl (188 kB)\n",
      "Using cached langsmith-0.0.69-py3-none-any.whl (48 kB)\n",
      "Using cached numpy-1.26.2-cp312-cp312-win_amd64.whl (15.5 MB)\n",
      "Using cached pydantic-2.5.2-py3-none-any.whl (381 kB)\n",
      "Using cached pydantic_core-2.14.5-cp312-none-win_amd64.whl (1.9 MB)\n",
      "Using cached SQLAlchemy-2.0.23-cp312-cp312-win_amd64.whl (2.1 MB)\n",
      "Using cached tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Using cached annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Using cached greenlet-3.0.2-cp312-cp312-win_amd64.whl (289 kB)\n",
      "Using cached marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached yarl-1.9.4-cp312-cp312-win_amd64.whl (76 kB)\n",
      "Building wheels for collected packages: frozenlist, multidict\n",
      "  Building wheel for frozenlist (pyproject.toml): started\n",
      "  Building wheel for frozenlist (pyproject.toml): finished with status 'error'\n",
      "  Building wheel for multidict (pyproject.toml): started\n",
      "  Building wheel for multidict (pyproject.toml): finished with status 'error'\n",
      "Failed to build frozenlist multidict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Building wheel for frozenlist (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [33 lines of output]\n",
      "  *********************\n",
      "  * Accelerated build *\n",
      "  *********************\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-312\n",
      "  creating build\\lib.win-amd64-cpython-312\\frozenlist\n",
      "  copying frozenlist\\__init__.py -> build\\lib.win-amd64-cpython-312\\frozenlist\n",
      "  running egg_info\n",
      "  writing frozenlist.egg-info\\PKG-INFO\n",
      "  writing dependency_links to frozenlist.egg-info\\dependency_links.txt\n",
      "  writing top-level names to frozenlist.egg-info\\top_level.txt\n",
      "  reading manifest file 'frozenlist.egg-info\\SOURCES.txt'\n",
      "  reading manifest template 'MANIFEST.in'\n",
      "  warning: no previously-included files matching '*.pyc' found anywhere in distribution\n",
      "  warning: no previously-included files matching '*.pyd' found anywhere in distribution\n",
      "  warning: no previously-included files matching '*.so' found anywhere in distribution\n",
      "  warning: no previously-included files matching '*.lib' found anywhere in distribution\n",
      "  warning: no previously-included files matching '*.dll' found anywhere in distribution\n",
      "  warning: no previously-included files matching '*.a' found anywhere in distribution\n",
      "  warning: no previously-included files matching '*.obj' found anywhere in distribution\n",
      "  warning: no previously-included files found matching 'frozenlist\\*.html'\n",
      "  no previously-included directories found matching 'docs\\_build'\n",
      "  adding license file 'LICENSE'\n",
      "  writing manifest file 'frozenlist.egg-info\\SOURCES.txt'\n",
      "  copying frozenlist\\__init__.pyi -> build\\lib.win-amd64-cpython-312\\frozenlist\n",
      "  copying frozenlist\\_frozenlist.pyx -> build\\lib.win-amd64-cpython-312\\frozenlist\n",
      "  copying frozenlist\\py.typed -> build\\lib.win-amd64-cpython-312\\frozenlist\n",
      "  running build_ext\n",
      "  building 'frozenlist._frozenlist' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for frozenlist\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Building wheel for multidict (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [74 lines of output]\n",
      "  *********************\n",
      "  * Accelerated build *\n",
      "  *********************\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-312\n",
      "  creating build\\lib.win-amd64-cpython-312\\multidict\n",
      "  copying multidict\\_abc.py -> build\\lib.win-amd64-cpython-312\\multidict\n",
      "  copying multidict\\_compat.py -> build\\lib.win-amd64-cpython-312\\multidict\n",
      "  copying multidict\\_multidict_base.py -> build\\lib.win-amd64-cpython-312\\multidict\n",
      "  copying multidict\\_multidict_py.py -> build\\lib.win-amd64-cpython-312\\multidict\n",
      "  copying multidict\\__init__.py -> build\\lib.win-amd64-cpython-312\\multidict\n",
      "  running egg_info\n",
      "  writing multidict.egg-info\\PKG-INFO\n",
      "  writing dependency_links to multidict.egg-info\\dependency_links.txt\n",
      "  writing top-level names to multidict.egg-info\\top_level.txt\n",
      "  reading manifest file 'multidict.egg-info\\SOURCES.txt'\n",
      "  reading manifest template 'MANIFEST.in'\n",
      "  warning: no previously-included files matching '*.pyc' found anywhere in distribution\n",
      "  warning: no previously-included files found matching 'multidict\\_multidict.html'\n",
      "  warning: no previously-included files found matching 'multidict\\*.so'\n",
      "  warning: no previously-included files found matching 'multidict\\*.pyd'\n",
      "  warning: no previously-included files found matching 'multidict\\*.pyd'\n",
      "  no previously-included directories found matching 'docs\\_build'\n",
      "  adding license file 'LICENSE'\n",
      "  writing manifest file 'multidict.egg-info\\SOURCES.txt'\n",
      "  C:\\Users\\sulayako\\AppData\\Local\\Temp\\pip-build-env-wu_keds_\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:207: _Warning: Package 'multidict._multilib' is absent from the `packages` configuration.\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          ############################\n",
      "          # Package would be ignored #\n",
      "          ############################\n",
      "          Python recognizes 'multidict._multilib' as an importable package[^1],\n",
      "          but it is absent from setuptools' `packages` configuration.\n",
      "  \n",
      "          This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "          package, please make sure that 'multidict._multilib' is explicitly added\n",
      "          to the `packages` configuration field.\n",
      "  \n",
      "          Alternatively, you can also rely on setuptools' discovery methods\n",
      "          (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "          instead of `find_packages(...)`/`find:`).\n",
      "  \n",
      "          You can read more about \"package discovery\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "  \n",
      "          If you don't want 'multidict._multilib' to be distributed and are\n",
      "          already explicitly excluding 'multidict._multilib' via\n",
      "          `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "          you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "          combination with a more fine grained `package-data` configuration.\n",
      "  \n",
      "          You can read more about \"package data files\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "  \n",
      "  \n",
      "          [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                even if it does not contain any `.py` files.\n",
      "                On the other hand, currently there is no concept of package data\n",
      "                directory, all directories are treated like packages.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    check.warn(importable)\n",
      "  copying multidict\\__init__.pyi -> build\\lib.win-amd64-cpython-312\\multidict\n",
      "  copying multidict\\py.typed -> build\\lib.win-amd64-cpython-312\\multidict\n",
      "  running build_ext\n",
      "  building 'multidict._multidict' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for multidict\n",
      "ERROR: Could not build wheels for frozenlist, multidict, which is required to install pyproject.toml-based projects\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in c:\\users\\sulayako\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.19.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\sulayako\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\sulayako\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub) (2023.12.0)\n",
      "Requirement already satisfied: requests in c:\\users\\sulayako\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\sulayako\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub) (4.66.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sulayako\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sulayako\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub) (4.8.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\sulayako\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\sulayako\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sulayako\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sulayako\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface_hub) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sulayako\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface_hub) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sulayako\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface_hub) (2023.11.17)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Using cached sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting transformers<5.0.0,>=4.6.0 (from sentence_transformers)\n",
      "  Obtaining dependency information for transformers<5.0.0,>=4.6.0 from https://files.pythonhosted.org/packages/0f/12/d8e27a190ca67811f81deea3183b528d9169f10b74d827e0b9211520ecfa/transformers-4.36.0-py3-none-any.whl.metadata\n",
      "  Using cached transformers-4.36.0-py3-none-any.whl.metadata (126 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sulayako\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence_transformers) (4.66.1)\n",
      "INFO: pip is looking at multiple versions of sentence-transformers to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting sentence_transformers\n",
      "  Using cached sentence-transformers-2.2.1.tar.gz (84 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "  Using cached sentence-transformers-2.2.0.tar.gz (79 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "  Using cached sentence-transformers-2.1.0.tar.gz (78 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting tokenizers>=0.10.3 (from sentence_transformers)\n",
      "  Using cached tokenizers-0.15.0.tar.gz (318 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Preparing metadata (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [6 lines of output]\n",
      "  \n",
      "  Cargo, the Rust package manager, is not installed or is not on PATH.\n",
      "  This package requires Rust and Cargo to compile extensions. Install it through\n",
      "  the system's package manager or via https://rustup.rs/\n",
      "  \n",
      "  Checking for Rust toolchain....\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "Encountered error while generating package metadata.\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Using cached faiss-cpu-1.7.4.tar.gz (57 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: faiss-cpu\n",
      "  Building wheel for faiss-cpu (pyproject.toml): started\n",
      "  Building wheel for faiss-cpu (pyproject.toml): finished with status 'error'\n",
      "Failed to build faiss-cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Building wheel for faiss-cpu (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [8 lines of output]\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  running build_ext\n",
      "  building 'faiss._swigfaiss' extension\n",
      "  swigging faiss\\faiss\\python\\swigfaiss.i to faiss\\faiss\\python\\swigfaiss_wrap.cpp\n",
      "  swig.exe -python -c++ -Doverride= -I/usr/local/include -Ifaiss -doxygen -DSWIGWIN -module swigfaiss -o faiss\\faiss\\python\\swigfaiss_wrap.cpp faiss\\faiss\\python\\swigfaiss.i\n",
      "  error: command 'swig.exe' failed: None\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for faiss-cpu\n",
      "ERROR: Could not build wheels for faiss-cpu, which is required to install pyproject.toml-based projects\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain\n",
    "!pip install huggingface_hub\n",
    "!pip install sentence_transformers\n",
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcsv_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CSVLoader\n\u001b[0;32m      3\u001b[0m loader \u001b[38;5;241m=\u001b[39m CSVLoader(file_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../Agriculture.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m documents \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mload()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain'"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "loader = CSVLoader(file_path='../Agriculture.csv')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I8ejWHh15FdA"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yZAoBkXy6yxQ",
    "outputId": "096831cf-a4ef-4387-bf39-7a0b06799cda"
   },
   "outputs": [],
   "source": [
    "# Check if the Hugging Face Hub API token is set\n",
    "import os\n",
    "\n",
    "api_token = os.environ.get(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "if api_token:\n",
    "    print(f\"Hugging Face Hub API token is set: {api_token}\")\n",
    "else:\n",
    "    print(\"Hugging Face Hub API token is not set. Please make sure to set it.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I9pWKmXl5THR",
    "outputId": "6c43b71e-7d8a-4df0-fc79-86c206b1ebf6"
   },
   "outputs": [],
   "source": [
    "# connect your Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount=True)\n",
    "\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "loader = CSVLoader(file_path='../Agriculture.csv')\n",
    "documents = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PsHYB3MUOUPm"
   },
   "outputs": [],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d-_cw8FEJagD"
   },
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def wrap_text_preserve_newlines(text, width=1110):\n",
    "    # Split the input text into lines based on newline characters\n",
    "    lines = text.split('\\n')\n",
    "\n",
    "    # Wrap each line individually\n",
    "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
    "\n",
    "    # Join the wrapped lines back together using newline characters\n",
    "    wrapped_text = '\\n'.join(wrapped_lines)\n",
    "\n",
    "    return wrapped_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4NjRpf5wO6NP",
    "outputId": "c708c112-e198-43b2-affb-674dab85ddef"
   },
   "outputs": [],
   "source": [
    "print(wrap_text_preserve_newlines(str(documents[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oGjh7ugVJdwD"
   },
   "outputs": [],
   "source": [
    "# Text Splitter\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size=10000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FscUNYdgO9A5",
    "outputId": "e900cdf4-59ba-4f85-aa64-8c6c5e246bc3"
   },
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oOEuRU9GPFbH",
    "outputId": "11afd9fb-a618-4b50-f959-24212295542e"
   },
   "outputs": [],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmUaKVmNI7fJ"
   },
   "source": [
    "#Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7CF8fGGTI6hx"
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rQUcqc0NJEtE"
   },
   "outputs": [],
   "source": [
    "# !pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8HPlaueNI_Uc"
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "db = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LcfckG8wJGaa"
   },
   "outputs": [],
   "source": [
    "query = \"What was the primary cropping system in Big Island, Hawaii, from 750 to 1400 CE?\"\n",
    "docs = db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2rHebBfcQQ8Y",
    "outputId": "1097a37a-aa31-46a7-ab96-07f5b2e2526a"
   },
   "outputs": [],
   "source": [
    "print(wrap_text_preserve_newlines(str(docs[0].page_content)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jrmXSEOhTBRm"
   },
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain import HuggingFaceHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i_YHaH_pTLH3",
    "outputId": "0361a15b-a8d2-4351-c255-ab0d014ec418"
   },
   "outputs": [],
   "source": [
    "llm=HuggingFaceHub(repo_id=\"google/flan-t5-xxl\", model_kwargs={\"temperature\":0.5, \"max_length\":1512})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uNB9c-_9Reny"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_answer_with_description(query, db, embeddings, num_results=1):\n",
    "    # Perform similarity search\n",
    "    results = db.similarity_search(query, num_results=num_results)\n",
    "\n",
    "    # Check if there are any results\n",
    "    if not results:\n",
    "        return \"No relevant answers found.\"\n",
    "\n",
    "    # Extract information from the top result\n",
    "    top_result = results[0]\n",
    "    page_content = top_result.page_content\n",
    "\n",
    "    # Generate embeddings for the query and the page content\n",
    "    query_embedding = embeddings.embed(query).reshape(1, -1)\n",
    "    page_content_embedding = embeddings.embed(page_content).reshape(1, -1)\n",
    "\n",
    "    # Calculate similarity score using cosine similarity\n",
    "    similarity_score = cosine_similarity(query_embedding, page_content_embedding)[0][0]\n",
    "\n",
    "    # Format the answer with a description\n",
    "    answer_with_description = f\"Answer: {wrap_text_preserve_newlines(str(page_content))}\\n\\n\" \\\n",
    "                              f\"Similarity Score: {similarity_score:.4f}\"\n",
    "\n",
    "    return answer_with_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rWfgl52CRsIK",
    "outputId": "4412ed3d-5f9e-46a1-f238-06ac0a3d7182"
   },
   "outputs": [],
   "source": [
    "query = \"What was the primary cropping system in Big Island, Hawaii, from 750 to 1400 CE?\"\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "answer_with_description = get_answer_with_description(query, db, embeddings)\n",
    "\n",
    "print(answer_with_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Lcn7tmjSJTS",
    "outputId": "b48d49c3-c0ba-4a46-b76c-f5d66453930a"
   },
   "outputs": [],
   "source": [
    "query = \" Historical Productivity range for Cahokia from 800 to 1500 CE?\"\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "answer_with_description = get_answer_with_description(query, db, embeddings)\n",
    "\n",
    "print(answer_with_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EY5-5esBStRH",
    "outputId": "3f89d990-363e-4b94-aacd-c7d8ea18b8d7"
   },
   "outputs": [],
   "source": [
    "query = \"Were fertilizers used in Cahokia around 1500 CE?\"\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "answer_with_description = get_answer_with_description(query, db, embeddings)\n",
    "\n",
    "print(answer_with_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZktPFfv1S7OZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
